\section{Related Work}\label{rw}
In this section techniques for computing MPC as a Quadratic Programming problem are discussed. A summary is then given of state-of-the-art FPGA-based hardware acceleration implementations for these techniques. We then position our approach within those works. 

%%MPC and QP
\textbf{\textit{Quadratic Programming (QP) solutions.}}
MPC can be posed as a Quadratic Programming problem in which a quadratic cost function is optimized subject to a set of linear equality and inequality constraints. As compared to computing Linear Quadratic Regulator (LQR) commands, which can be posed as a QP problem that is subject to only linear equalities, computing MPC commands require vastly more computing resources. There are two key reasons for this. First, when only equality constraints are considered there is an analytical solution, while iterative methods are required once inequality constraints are introduced. Second, LQR only uses the current system state and sensor inputs to compute its next actuator command, while MPC additionally uses predictions of system state and sensor inputs over a specified number of time steps into the future (i.e. prediction horizon)~\cite{Kwakernaak:1972:LOC:578807}.

QP problems can be solved reliably via various iterative methods. Three common methods are the: 1) Interior-Point Method (IPM)~\cite[Chapter~4.3.2]{borrelli2015predictive}, 2) the Active Set Method (ASM)~\cite[Chapter~4.3.3]{borrelli2015predictive}, and 3) Alternating Directions Method of Multipliers (ADMM)~\cite{boyd2011distributed,6422363}. For IPM, each inequality constraint is transformed into a sequence of equality constrained problems, and solved using Newton's method. ASM selects a subset of all specified inequalities based on which inequalities are currently "active", meaning they will affect the optimization result at the current stage of the computation. ADMM (also called operator splitting) allows large QP problems to be broken into a set of smaller pieces, thus allowing for more opportunities for parallelism. A detailed description of IPM and ASM can be found in~\cite{Nocedal2006NO}, ~\cite{boyd2011distributed} gives a comprehensive introduction to ADMM, and a good reference for solutions to the more general problem of convex optimization is~\cite{Boyd993483}.

In most cases, IPM requires fewer iterations than ASM to converge. However, each iteration of IPM is more computationally expensive because it solves a linear system involving all the variables of the problem, whereas ASM solves linear systems involving a subset of all the variables (i.e. variables associated with a subset of active constraints). ADMM converges slower than IPM and ASM to achieve the same accuracy, while each iteration is easier to compute.

%%FPGAs and QP
\textbf{\textit{FPGA-based QP solutions.}}
Several works have investigated accelerating QP solutions for the purpose of MPC\cite{5681439,6376494,6927473,wills2011fpga,7074396,jerez2014embedded,7331067}. Hardware acceleration of MPC using IPM was performed by~\cite{5681439,6376494,wills2011fpga,6927473}. For these works accelerating the linear equation solver required for IPM was the focus. In~\cite{5681439}, the MINRES algorithm was used to exploit potential parallelism within the linear equation solver. In~\cite{6376494} and~\cite{wills2011fpga}, acceleration of a Conjugate Gradient Method based linear solver was conducted. In~\cite{6927473}, the linear equation solver used a Cholesky decomposition approach that enabled implementing a predictor-corrector that reduced the number of solver iterations. An accelerator using the ASM approach was implemented in~\cite{7074396}. Additionally,~\cite{7074396} examined the trade-offs between using an ASM verses an IPM approach. They concluded that ASM gives lower computing complexity and converges faster when the number of decision variables and constraints are small. Otherwise, IPM is a better choice when considering scalability.

ADMM's inherent parallelizability makes it a natural fit for hardware acceleration. Two works that have developed ADMM acceleration engines are~\cite{jerez2014embedded} and~\cite{7331067}. In~\cite{jerez2014embedded}, a highly parallel architecture was presented, and the tradeoff between accuracy and computing resources when using custom fixed-point number representation within the engine's core was a major focus. The high-level architecture of this work's computing core is the most similar to our work. In~\cite{7331067}, the use of a sparse QP formulation under polytopic constraints was the primary contribution.

For our ADMM-based MPC acceleration engine, we have focused on a SW/HW co-design that is flexible and eases its use and deployment into a system. In terms of flexibility, our architecture allows scaling in such a way that computing speed can be traded off for hardware resources, enabling relatively large controllers to be deployed when only a small number of on-chip resources can be allocated to the engine. Regarding ease of use and deployment, we have tightly integrated our MPC engine with an on-chip ARM processor and we implement standard 32-bit floating point computations. Software running on the ARM processor makes updating the MPC engine with a new controller convenient, and our hardware architecture has implemented software settable ADMM tuning features as well.
